{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Installing and importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:01:00.989218Z","iopub.status.busy":"2024-09-04T20:01:00.988653Z","iopub.status.idle":"2024-09-04T20:01:19.927783Z","shell.execute_reply":"2024-09-04T20:01:19.926645Z","shell.execute_reply.started":"2024-09-04T20:01:00.989173Z"},"trusted":true},"outputs":[],"source":["pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install accelerate"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:01:33.006953Z","iopub.status.busy":"2024-09-04T20:01:33.006170Z","iopub.status.idle":"2024-09-04T20:01:38.155945Z","shell.execute_reply":"2024-09-04T20:01:38.155125Z","shell.execute_reply.started":"2024-09-04T20:01:33.006907Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","import json\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:01:38.158295Z","iopub.status.busy":"2024-09-04T20:01:38.157805Z","iopub.status.idle":"2024-09-04T20:01:38.197632Z","shell.execute_reply":"2024-09-04T20:01:38.196020Z","shell.execute_reply.started":"2024-09-04T20:01:38.158260Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the model in Quantized format"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:01:38.199090Z","iopub.status.busy":"2024-09-04T20:01:38.198758Z","iopub.status.idle":"2024-09-04T20:02:45.139949Z","shell.execute_reply":"2024-09-04T20:02:45.138824Z","shell.execute_reply.started":"2024-09-04T20:01:38.199056Z"},"trusted":true},"outputs":[],"source":["nf4_config = BitsAndBytesConfig(\n","   load_in_4bit=True,\n","   bnb_4bit_quant_type=\"nf4\",\n","   bnb_4bit_use_double_quant=True,\n","   bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","tokenizer = AutoTokenizer.from_pretrained('shlokjain0177/SuperiorLLM', trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\"shlokjain0177/SuperiorLLM\",quantization_config=nf4_config, device_map=\"auto\", trust_remote_code=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('/datasets/world_facts_qna.json') as f:\n","    dataset_facts = json.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["### Function to get text embeddings from the model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:03:38.758580Z","iopub.status.busy":"2024-09-04T20:03:38.757807Z","iopub.status.idle":"2024-09-04T20:03:38.765016Z","shell.execute_reply":"2024-09-04T20:03:38.763831Z","shell.execute_reply.started":"2024-09-04T20:03:38.758536Z"},"trusted":true},"outputs":[],"source":["import torch\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def get_llm_embedding(text):\n","    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_hidden_states=True)\n","    embeddings = outputs.hidden_states[-1].mean(dim=1)\n","    return embeddings.squeeze().numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### Function to get cosine similarity between two text embeddings"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T20:03:40.477755Z","iopub.status.busy":"2024-09-04T20:03:40.477015Z","iopub.status.idle":"2024-09-04T20:03:40.483261Z","shell.execute_reply":"2024-09-04T20:03:40.482294Z","shell.execute_reply.started":"2024-09-04T20:03:40.477713Z"},"trusted":true},"outputs":[],"source":["def cosine_similarity_llm(generated_answer, actual_answer):\n","    # Get embeddings for both answers\n","    generated_embedding = get_llm_embedding(generated_answer)\n","    actual_embedding = get_llm_embedding(actual_answer)\n","\n","    generated_embedding = generated_embedding.reshape(1, -1)\n","    actual_embedding = actual_embedding.reshape(1, -1)\n","    \n","    cosine_sim = cosine_similarity(generated_embedding, actual_embedding)\n","    return cosine_sim[0][0]"]},{"cell_type":"markdown","metadata":{},"source":["### Function to generate probability scores for given question answer pairs"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-09-04T21:51:36.189640Z","iopub.status.busy":"2024-09-04T21:51:36.189233Z","iopub.status.idle":"2024-09-04T21:51:36.198751Z","shell.execute_reply":"2024-09-04T21:51:36.197819Z","shell.execute_reply.started":"2024-09-04T21:51:36.189601Z"},"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","\n","def bleu_score(generated_answer, actual_answer):\n","    reference = [actual_answer.lower().split()]\n","    candidate = generated_answer.lower().split()\n","    score = sentence_bleu(reference, candidate)\n","    return score\n","\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n","\n","def get_probability_score(question, answer):\n","    prompt = f\"<s>[INST] Answer the question in ONE WORD. [/INST] Question: {question} \\n Answer: \"\n","    result = pipe(prompt)\n","    generated_answer = result[0]['generated_text'].split(\"Answer: \")[1].strip()\n","    generated_answer = generated_answer.split()\n","    generated_ans = ''\n","    if len(generated_answer)>=len(answer.split()):\n","        generated_ans = \" \".join(generated_answer[0:len(answer.split())])\n","    else:\n","        generated_ans = \" \".join(generated_answer)\n","    probability_score = cosine_similarity_llm(generated_ans, answer)\n","    return probability_score, generated_ans"]},{"cell_type":"markdown","metadata":{},"source":["### Calculating average probability for the whole dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-04T20:05:42.486378Z","iopub.status.busy":"2024-09-04T20:05:42.485451Z","iopub.status.idle":"2024-09-04T20:30:02.567618Z","shell.execute_reply":"2024-09-04T20:30:02.566656Z","shell.execute_reply.started":"2024-09-04T20:05:42.486336Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["avg = 0\n","print(len(dataset_facts))\n","for i, data in enumerate(dataset_facts):\n","    question = data[\"question\"]\n","    answer = data[\"answer\"]\n","    prob_score, generated_answer = get_probability_score(question, answer)\n","    avg += prob_score\n","    print(i)\n","print(avg/len(dataset_facts))\n","    "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
