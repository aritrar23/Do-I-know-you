{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ab2f0f",
   "metadata": {},
   "source": [
    "# EasyEdit Example with **MEMIT** on SuperLLM\n",
    "We implement MEMIT in the following notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259f06e",
   "metadata": {},
   "source": [
    "Method:MEMIT\n",
    "Paper:[MASS-EDITING MEMORY IN A TRANSFORMER](https://arxiv.org/abs/2210.07229)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b839033",
   "metadata": {},
   "source": [
    "## Prepare the runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b7da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wmr/EasyEdit\n",
      "data\t    figs\t hugging_cache\tREADME.md\t  tutorial-notebooks\r\n",
      "easyeditor  globals.yml  LICENSE\trequirements.txt\r\n",
      "edit.py     hparams\t logs\t\tresults\r\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/zjunlp/EasyEdit\n",
    "%cd EasyEdit\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install python3.9\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
    "!sudo update-alternatives --config python3\n",
    "!apt-get install python3-pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135a608",
   "metadata": {},
   "source": [
    "## Config Method Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912a228",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "# For MEMIT hparams:\n",
    "alg_name: \"MEMIT\"\n",
    "model_name: \"./hugging_cache/llama-7b\"\n",
    "device: 0\n",
    "layers: [4, 5, 6, 7]\n",
    "clamp_norm_factor: 0.75\n",
    "layer_selection: \"all\"\n",
    "fact_token: \"subject_last\"\n",
    "v_num_grad_steps: 20\n",
    "v_lr: 5e-1\n",
    "v_loss_layer: 31\n",
    "v_weight_decay: 0.5\n",
    "kl_factor: 0.0625\n",
    "mom2_adjustment: true\n",
    "mom2_update_weight: 20000\n",
    "rewrite_module_tmp: \"model.layers.{}.mlp.down_proj\"\n",
    "layer_module_tmp: \"model.layers.{}\"\n",
    "mlp_module_tmp: \"model.layers.{}.mlp\"\n",
    "attn_module_tmp:  \"model.layers.{}.self_attn\"\n",
    "ln_f_module: \"model.norm\"\n",
    "lm_head_module: \"lm_head\"\n",
    "mom2_dataset: \"wikipedia\"\n",
    "mom2_n_samples: 100000\n",
    "mom2_dtype: \"float32\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2181cd",
   "metadata": {},
   "source": [
    "## Import modules & Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818879db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyeditor import BaseEditor\n",
    "from easyeditor import MEMITHyperParams\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12ea423",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams=MEMITHyperParams.from_hparams('./hparams/MEMIT/llama-7b.yaml')\n",
    "\n",
    "prompts = ['Who was the designer of Lahti Town Hall?',\n",
    "                'What role does Denny Herzig play in football?',\n",
    "                'What city did Marl Young live when he died?']\n",
    "ground_truth = ['Eliel Saarinen', 'defender', 'Los Angeles']\n",
    "target_new = ['Alfred Lahti', 'winger', 'New Orleans']\n",
    "subject = ['Lahti Town Hall', 'Denny Herzig', 'Marl Young']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor=BaseEditor.from_hparams(hparams)\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    keep_original_weight=False\n",
    ")\n",
    "print(metrics)\n",
    "print(type(edited_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee2632",
   "metadata": {},
   "source": [
    "#### Reliability Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM\n",
    "tokenizer = LlamaTokenizer.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side='left'\n",
    "\n",
    "correct_prompts = ['Who was the designer of Lahti Town Hall?',\n",
    "                'What role does Denny Herzig play in football?',\n",
    "                'What city did Marl Young live when he died?']\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache').to('cuda')\n",
    "batch = tokenizer(correct_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    ")\n",
    "print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dcef9",
   "metadata": {},
   "source": [
    "#### Generalization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49753a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:  ['<unk><unk><unk>Who was the architect behind the design of Lahti Town Hall? Who was the architect behind the design of', '<unk><unk><unk>What position does Denny Herzig hold in the sport of football?\\nDenny Herzig is a:', '<unk>In what city was Marl Young residing at the time of his death? 10. In what city was']\n",
      "Post-Edit Outputs:  ['<unk><unk><unk>Who was the architect behind the design of Lahti Town Hall?\\n10. Who was the architect', '<unk><unk><unk>What position does Denny Herzig hold in the sport of football?\\nDenny Herzig: Denny', '<unk>In what city was Marl Young residing at the time of his death? New Orleans, Louisiana. What was the']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import LlamaTokenizer\n",
    "# from transformers import LlamaForCausalLM\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "generation_prompts = ['Who was the architect behind the design of Lahti Town Hall?',\n",
    "'What position does Denny Herzig hold in the sport of football?',\n",
    "'In what city was Marl Young residing at the time of his death?']\n",
    "\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache').to('cuda')\n",
    "batch = tokenizer(generation_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    ")\n",
    "print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5cb84",
   "metadata": {},
   "source": [
    "#### Locality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9029f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Edit Outputs:  ['<unk><unk>Who was the designer of Eiffel Tower?\\n10. Who was the designer', '<unk><unk><unk>What role does Messi play in football?\\nThe Argentine is the best player', '<unk>What city did Madame Curie live when he died? 10. What city did Madame']\n",
      "Post-Edit Outputs:  ['<unk><unk>Who was the designer of Eiffel Tower?\\n10. Who was the designer', '<unk><unk><unk>What role does Messi play in football?\\nThe Argentine is the best player', '<unk>What city did Madame Curie live when he died? 10. What city did Madame']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import LlamaTokenizer\n",
    "# from transformers import LlamaForCausalLM\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache')\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# tokenizer.padding_side='left'\n",
    "\n",
    "\n",
    "locality_prompts = ['Who was the designer of Eiffel Tower?',\n",
    "                'What role does Messi play in football?',\n",
    "                'What city did Madame Curie live when he died?']\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained('./hugging_cache/llama-7b', cache_dir='./hugging_cache').to('cuda')\n",
    "batch = tokenizer(locality_prompts, return_tensors='pt', padding=True, max_length=30)\n",
    "\n",
    "pre_edit_outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "post_edit_outputs = edited_model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "#     max_length=15\n",
    "    max_new_tokens=8\n",
    ")\n",
    "print('Pre-Edit Outputs: ', [tokenizer.decode(x) for x in pre_edit_outputs.detach().cpu().numpy().tolist()])\n",
    "print('Post-Edit Outputs: ', [tokenizer.decode(x) for x in post_edit_outputs.detach().cpu().numpy().tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff94ad",
   "metadata": {},
   "source": [
    "This approach failed due to resource constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "easyedit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
